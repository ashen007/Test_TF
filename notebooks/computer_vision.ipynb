{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ***Neurons for Vision***\n",
    "\n",
    "The first, Flatten, isn’t a layer of neurons, but an input layer specification. Our\n",
    "inputs are 28 × 28 images, but we want them to be treated as a series of numeric values.\n",
    "\n",
    "The next one, Dense, is a layer of neurons, and we’re specifying that we want 128 of\n",
    "them. This is the middle layer. You’ll often hear such layers described as hidden layers.\n",
    "Layers that are between the inputs and the outputs aren’t seen by a caller, so the term\n",
    "“hidden” is used to describe them. We’re asking for 128 neurons to have their internal\n",
    "parameters randomly initialized. Often the question I’ll get asked at this point is\n",
    "“Why 128?” This is entirely arbitrary—there’s no fixed rule for the number of neurons\n",
    "to use. As you design the layers you want to pick the appropriate number of values to\n",
    "enable your model to actually learn. More neurons means it will run more slowly, as it\n",
    "has to learn more parameters. More neurons could also lead to a network that is great at\n",
    "recognizing the training data, but not so good at recognizing data that it hasn’t previously\n",
    "seen. On the other hand, fewer neurons' means that the model might not have sufficient\n",
    "parameters to learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "****note****\n",
    "\n",
    "***The activation function is code that will execute on each neuron in the layer.\n",
    "TensorFlow supports a number of them, but a very common one in middle layers is relu, which\n",
    "stands for rectified linear unit. It’s a simple function that just returns a value if it’s\n",
    "greater than 0. In this case, we don’t want negative values being passed to the next\n",
    "layer to potentially impact the summing function, so instead of writing a lot of\n",
    "if-then code, we can simply activate the layer with relu.***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flatter = Flatten(input_shape=(28, 28))\n",
    "layer_1 = Dense(128, activation=tf.nn.relu)\n",
    "layer_2 = Dense(10, activation=tf.nn.softmax)\n",
    "\n",
    "model = Sequential(layers=[])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}